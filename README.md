# Gatetunbal_neuron_in_early_fusion_network_V2
Demo code for a paper--Gate tunable MoS2 memristive neuron for early fusion multimodal spiking neural network
## Summarize
The early fusion multimodal leraning use FSDD and MNIST as the multimodal dataset. Unlike normal CNN, SNN has a time dimension. We use a 128 steps simulation for each input data of MNIST or FSDD. We train this neual network with two dataset at the same time. In our paper, we mentioned that neuron memristors have different fire ratio and fire current under different back gate voltage. We give different neuron different activation based on whether it is MNIST or FSDD. This new early fusion neural network gives better performance than previous research and save the hardware cost as well as the power consumption. You can get deeper insights in our article and supplementary information.
## Dataset
0-9 digital images come from MNIST dataset (Neural Computation, 1(4): 541-551, 1989), which is already widely used in neural networks. We take 6000 images for training and 1000 images for testing. And 0-9 digital audios come from Free-Spoken-Digit-Dataset (https://github.com/Jakobovski/free-spoken-digit-dataset/). This dataset has also been used in multiple papers. It contains 1500 audios from differnt speakers for number 0-9 and we take 1000 of them as training set and 500 as testing set randomly. By adding additive white gaussian noise, we expand training set to 6000 and testing set to 1000, the same as the images dataset. Utilizing STFT(short-time fourier transfer), we transfer them into spectrograms to extract the features of these audios. Last, we downsample them to obtain a 28x28 pixel  spectrogram image with 8-bit grayscale depth value. These data decide the input voltage of the the network. The method of processing audio dataset comes from a published conference paper (ECCV 2018, vol 11134, Springer, Cham) in which the authors created a dataset called Audiovisual MNIST.  
`Test_audio_data.pt`,`Test_audio_label.pt`,`Train_audio_data.pt`and`Train_audio_label.pt` are the audio dataset. And the iamge dataset can be download automatically.
## Threshold adjustable pulse characteristics of nueron model
Based on the electrical characteristics of real devices, Matlab physical models, and Hspice electrical models, we modify our neuron model based on LIF (leaky-integrity-fire) Model. LIF Model has three types of neuron behaviors which are similar to biological neurons. First, When the neuron does not receive the front-end pulse, the source end charge will slowly leak out, Causing a slow decrease in top-electrode voltage (the 'slow' here is only relative to the rapid increase in potential with pulses, which can actually decrease to near zero potential within a few milliseconds). Second, neurons will receive pulses from all front-end synapses connected to the neuron, and the accumulation of these pulses determines that the top-electrode voltage can or cannot reach the threshold voltage. Third, once the top-electrode voltage reaches the threshold voltage, it will instantly become conductive and release a pulse to the post-end synapse. In the neurons studied in this study, the threshold and pulse size are determined by the gate voltage. In our SNN simulation, half of the neurons are in the high gate volatge (0V), while the other half are in the low gate voltage(-1.5V), with a discharge current difference of nearly 8 times, which is very suitable for our model.  
We set up a neural network containing only one neuron to adjust the parameters to approximate the test results of real devices, and finally this neuron performs as follows under different gate voltages:
`tips`:Set white background color to view the following image.
![image](https://github.com/Peng-Yuyang-2003/Gatetunbal_neuron_in_early_fusion_network_V2/blob/main/single_neuron.png)
By comparing with the device test result in the paper, we conclude that the simulation result are very similar to it.  
`main.py` is the code we train the SNN and use the SNN reasoning.
## 4 bit precision memristor model
In our neuron network simulation, we use a 4 bit precision memristor model to present 16 different weights. As we don’t mainly focus on synapses, we cites others people’s work to demonstrate this simulation assumption is feasible. Yao, P. et al. (Nature 577, 641–646, 2020) has implemented a memristor-based neural network with 15-level weights. Their memristor synapses have been implemented in hardware. Positive and negative weights have been implemented through differential pairs. We first trained our SNN with the float32 data type and then quantify the weight to 4 bits. Finally, we briefly trained the neural network to adapt to low precision weights of 4 bits, and obtained that the accuracy loss of the neural network compared to the float32 data type is no more than 3%.  
`quantize_the_weights.py`is the code to quantize the weights in SNN. `spiking_model_quantized_weights.pth`and`spiking_model_weights.pth`are the weights we hae already trained.
## How to use the neural network reasoning
1. Download all files.
2. Configure the environment with python=3.12.7 and torch=2.5.1+cu124, as well as installing the rest of the packages. You can also use the requirements.txt to configuring the software environment all at once.
3. Run main.py and wait for the output. Test set accuracy is around 91%.
## How to train the neural network
### first
